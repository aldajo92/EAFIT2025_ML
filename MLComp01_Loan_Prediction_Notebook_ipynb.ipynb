{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6Ah_3xfVgyL"
      },
      "source": [
        "# Loan Prediction Competition\n",
        "\n",
        "In this workshop, we will apply ensemble methods such as Random Forest and Gradient Boosting to a loan prediction dataset. The dataset used is a modified subset of the **Loan Prediction Problem Dataset** from Kaggle ([link](https://www.kaggle.com/datasets/altruistdelhite04/loan-prediction-problem-dataset)).\n",
        "\n",
        "## Objective\n",
        "The task is to predict whether a loan application will be approved based on applicant information.\n",
        "\n",
        "![Loan Prediction Competition](https://drive.google.com/uc?id=1eipuAdG46mfAgm-KSFth_YEazhJAZHVx)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-08LIEYiYCYd"
      },
      "source": [
        "## Loading the Data\n",
        "\n",
        "The training dataset is loaded from the **[train.csv](https://drive.google.com/file/d/1Ejs0yaRm3NxFOVIhwQphoDz8voJl6NQx/view?usp=sharing)** file using Pandas. After loading, we inspect the first few rows to understand its structure and check for missing values. Basic preprocessing steps, such as handling null values and encoding categorical variables, will be performed before modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xTuFAU9QDh2"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mInvalid response: 404 Not Found. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJDXpBtyZr1J"
      },
      "outputs": [],
      "source": [
        "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Ejs0yaRm3NxFOVIhwQphoDz8voJl6NQx' -O loanpred_train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyZJf3QPaCgL",
        "outputId": "dd9cd585-0f63-403f-e493-2aa1136b4ad2"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1prsz-DTYi9cfnzHiPU--Nr2pT-832pTg' -O loanpred_train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL2DLKnraE6f"
      },
      "outputs": [],
      "source": [
        "# Let's define the \"random_state\" to ensure reproducible results:\n",
        "random_state=42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oucEQ4RCaVfx"
      },
      "outputs": [],
      "source": [
        "# Let's change the font of Matplotlib plots:\n",
        "plt.rc('font', family='serif', size=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "nVghSfMVajKX",
        "outputId": "3430b42c-ee5f-4bba-b950-9083ae08cc47"
      },
      "outputs": [],
      "source": [
        "#Let's load the data\n",
        "# Carguemos los datos:\n",
        "data = pd.read_csv('loanpred_train.csv')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2icEvBRa9hI"
      },
      "outputs": [],
      "source": [
        "# Let's check the dataset description:\n",
        "print(\"Dataset Shape:\", data.shape)\n",
        "print(\"\\nDataset Info:\")\n",
        "data.info()\n",
        "print(\"\\nDataset Description:\")\n",
        "data.describe()\n",
        "print(\"\\nMissing Values:\")\n",
        "print(data.isnull().sum())\n",
        "print(\"\\nTarget Variable Distribution:\")\n",
        "print(data['Loan_Status'].value_counts())\n",
        "print(data['Loan_Status'].value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GmNxB89bFnq"
      },
      "outputs": [],
      "source": [
        "# Let's check the distribution of the columns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create subplots for visualizations\n",
        "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
        "fig.suptitle('Data Distribution Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Categorical variables\n",
        "categorical_cols = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Status']\n",
        "for i, col in enumerate(categorical_cols):\n",
        "    row = i // 4\n",
        "    col_idx = i % 4\n",
        "    data[col].value_counts().plot(kind='bar', ax=axes[row, col_idx], color='skyblue')\n",
        "    axes[row, col_idx].set_title(f'Distribution of {col}')\n",
        "    axes[row, col_idx].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Numerical variables\n",
        "numerical_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    row = (i + len(categorical_cols)) // 4\n",
        "    col_idx = (i + len(categorical_cols)) % 4\n",
        "    if row < 3 and col_idx < 4:  # Make sure we don't exceed subplot limits\n",
        "        data[col].hist(bins=20, ax=axes[row, col_idx], color='lightcoral', alpha=0.7)\n",
        "        axes[row, col_idx].set_title(f'Distribution of {col}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "# Select only numerical columns for correlation\n",
        "numerical_data = data.select_dtypes(include=[np.number])\n",
        "correlation_matrix = numerical_data.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10J9JbUtbshJ"
      },
      "source": [
        "## Data Preprocessing Pipeline\n",
        "\n",
        "Before modeling, we preprocess the dataset by handling missing values, encoding categorical variables, and scaling numerical features if necessary. This ensures that the data is clean and properly formatted for training machine learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JkqZRZobWj7"
      },
      "outputs": [],
      "source": [
        "# Let's complete the data analysis stage and define the preprocessing pipeline\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import joblib\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(['id', 'Loan_Status'], axis=1)\n",
        "y = data['Loan_Status']\n",
        "\n",
        "# Encode target variable (Y=1, N=0)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(\"Target encoding:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_features = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area']\n",
        "numerical_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']\n",
        "\n",
        "print(f\"Categorical features: {categorical_features}\")\n",
        "print(f\"Numerical features: {numerical_features}\")\n",
        "\n",
        "# Create preprocessing pipelines\n",
        "# For numerical features: impute missing values with median and scale\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# For categorical features: impute missing values with most frequent and one-hot encode\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(drop='first', sparse_output=False))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Preprocessing pipeline created successfully!\")\n",
        "\n",
        "# FEATURE ENGINEERING - Add new features to improve predictions\n",
        "def add_engineered_features(df):\n",
        "    \"\"\"Add engineered features based on domain knowledge\"\"\"\n",
        "    df_new = df.copy()\n",
        "    \n",
        "    # 1. Total Income (most important feature)\n",
        "    df_new['TotalIncome'] = df_new['ApplicantIncome'] + df_new['CoapplicantIncome']\n",
        "    \n",
        "    # 2. Loan Amount to Income Ratio (critical for loan approval)\n",
        "    df_new['LoanAmountToIncome'] = df_new['LoanAmount'] / (df_new['TotalIncome'] + 1)  # +1 to avoid division by zero\n",
        "    \n",
        "    # 3. Income per dependent\n",
        "    df_new['IncomePerDependent'] = df_new['TotalIncome'] / (df_new['Dependents'].astype(str).replace('3+', '3').astype(float) + 1)\n",
        "    \n",
        "    # 4. Loan term in years\n",
        "    df_new['LoanTermYears'] = df_new['Loan_Amount_Term'] / 12\n",
        "    \n",
        "    # 5. Monthly EMI approximation (Loan Amount / Term in months)\n",
        "    df_new['MonthlyEMI'] = df_new['LoanAmount'] / (df_new['Loan_Amount_Term'] + 1)\n",
        "    \n",
        "    # 6. EMI to Income ratio\n",
        "    df_new['EMIToIncomeRatio'] = (df_new['MonthlyEMI'] * 1000) / (df_new['TotalIncome'] + 1)  # *1000 because LoanAmount is in thousands\n",
        "    \n",
        "    # 7. Binary features\n",
        "    df_new['HasCoapplicant'] = (df_new['CoapplicantIncome'] > 0).astype(int)\n",
        "    df_new['HasDependents'] = (df_new['Dependents'].astype(str) != '0').astype(int)\n",
        "    \n",
        "    return df_new\n",
        "\n",
        "# Apply feature engineering\n",
        "print(\"Adding engineered features...\")\n",
        "X_engineered = add_engineered_features(X)\n",
        "\n",
        "# Update feature lists\n",
        "additional_numerical_features = ['TotalIncome', 'LoanAmountToIncome', 'IncomePerDependent', \n",
        "                               'LoanTermYears', 'MonthlyEMI', 'EMIToIncomeRatio', \n",
        "                               'HasCoapplicant', 'HasDependents']\n",
        "\n",
        "numerical_features_extended = numerical_features + additional_numerical_features\n",
        "\n",
        "print(f\"Original features: {len(numerical_features + categorical_features)}\")\n",
        "print(f\"With engineered features: {len(numerical_features_extended + categorical_features)}\")\n",
        "\n",
        "# Update preprocessing pipeline with new features\n",
        "numerical_transformer_extended = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor_extended = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer_extended, numerical_features_extended),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_engineered, y_encoded, test_size=0.2, random_state=random_state, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "print(f\"Training target distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test target distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "# Update the global preprocessor variable\n",
        "preprocessor = preprocessor_extended\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xgmwEdhdDk0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VttUV3Iyae0e"
      },
      "source": [
        "# Model Training and Evaluation  \n",
        "\n",
        "We train machine learning models, such as Random Forest, Gradient Boosting or XGBoost, to predict loan approval. The models are evaluated using appropriate metrics, and hyperparameter tuning is performed to optimize their performance.\n",
        "\n",
        "In this section, we define the steps for training and evaluating the models.  \n",
        "\n",
        "## Steps:  \n",
        "1. **Define the hyperparameters**: Set initial values for model parameters.  \n",
        "2. **Choose the cross-validation strategy**: Split the dataset into training and validation sets using an appropriate method.  \n",
        "3. **Train the model**: Fit the model on training data using the defined hyperparameters.  \n",
        "4. **Evaluate performance**: Use cross-validation to assess the model’s predictive ability.  \n",
        "5. **Tune hyperparameters (if necessary)**: Optimize parameters for better performance.  \n",
        "\n",
        "## Hyperparameters  \n",
        "We define key hyperparameters for Random Forest and Gradient Boosting models, such as:  \n",
        "- **n_estimators**: Number of trees in the ensemble.  \n",
        "- **max_depth**: Maximum depth of each tree.  \n",
        "- **learning_rate** (for boosting models): Controls step size for weight updates.  \n",
        "- **min_samples_split**: Minimum samples required to split a node.  \n",
        "- **min_samples_leaf**: Minimum samples required in a leaf node.  \n",
        "\n",
        "## Cross-Validation Strategy  \n",
        "To ensure reliable model evaluation, we use **K-Fold Cross-Validation**, which splits the dataset into **K** subsets (folds). The model is trained on **K-1** folds and tested on the remaining fold, repeating the process **K** times. This helps in reducing variance and providing a better generalization estimate.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxyRfUDDb-Co"
      },
      "outputs": [],
      "source": [
        "# Let's define the cross_validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Try to import XGBoost for potentially better performance\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    xgb_available = True\n",
        "    print(\"XGBoost is available - adding to model comparison\")\n",
        "except ImportError:\n",
        "    xgb_available = False\n",
        "    print(\"XGBoost not available - using only RandomForest and GradientBoosting\")\n",
        "\n",
        "# Define cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "\n",
        "# Define models with hyperparameters to tune\n",
        "models = {\n",
        "    'RandomForest': {\n",
        "        'model': RandomForestClassifier(random_state=random_state),\n",
        "        'params': {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__max_depth': [3, 5, 7, None],\n",
        "            'classifier__min_samples_split': [2, 5, 10],\n",
        "            'classifier__min_samples_leaf': [1, 2, 4]\n",
        "        }\n",
        "    },\n",
        "    'GradientBoosting': {\n",
        "        'model': GradientBoostingClassifier(random_state=random_state),\n",
        "        'params': {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__learning_rate': [0.05, 0.1, 0.2],\n",
        "            'classifier__max_depth': [3, 5, 7],\n",
        "            'classifier__min_samples_split': [2, 5, 10],\n",
        "            'classifier__min_samples_leaf': [1, 2, 4]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Add XGBoost if available\n",
        "if xgb_available:\n",
        "    models['XGBoost'] = {\n",
        "        'model': XGBClassifier(random_state=random_state, eval_metric='logloss'),\n",
        "        'params': {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__learning_rate': [0.05, 0.1, 0.2],\n",
        "            'classifier__max_depth': [3, 4, 5, 6],\n",
        "            'classifier__min_child_weight': [1, 3, 5],\n",
        "            'classifier__subsample': [0.8, 0.9, 1.0],\n",
        "            'classifier__colsample_bytree': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Function to evaluate models with cross-validation\n",
        "def evaluate_model_cv(model, X, y, cv_strategy, preprocessor):\n",
        "    \"\"\"Evaluate model using cross-validation\"\"\"\n",
        "    # Create pipeline with preprocessing and model\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    \n",
        "    # Perform cross-validation\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=cv_strategy, scoring='accuracy')\n",
        "    \n",
        "    return cv_scores\n",
        "\n",
        "# Quick evaluation of base models\n",
        "print(\"=== Base Model Cross-Validation Results ===\")\n",
        "for model_name, model_info in models.items():\n",
        "    model = model_info['model']\n",
        "    cv_scores = evaluate_model_cv(model, X_train, y_train, cv_strategy, preprocessor)\n",
        "    \n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"CV Scores: {cv_scores}\")\n",
        "    print(f\"Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "print(\"\\nCross-validation strategy defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrSoo7_QdF8q"
      },
      "source": [
        "# Finalizing the Model: Training a Full Pipeline  \n",
        "\n",
        "Once the best model parameters have been selected, it is essential to **train a complete pipeline** that explicitly separates **data preprocessing** and **model training**. This ensures that preprocessing steps are consistently applied to both training and unseen data.  \n",
        "\n",
        "## Steps:  \n",
        "1. **Train the data preprocessing pipeline**:  \n",
        "   - Handle missing values.  \n",
        "   - Encode categorical features.  \n",
        "   - Scale numerical features (if necessary).  \n",
        "\n",
        "2. **Train the classification pipeline**:  \n",
        "   - Use the entire processed training dataset and the best model hyperparameters to fit the selected model to make final predictions.  \n",
        "\n",
        "3. **Save the trained pipelines**:  \n",
        "   - The preprocessing and classification models should be saved for deployment and inference.  \n",
        "\n",
        "By structuring the pipeline this way, we maintain consistency between training and real-world predictions while ensuring that preprocessing does not introduce **data leakage**.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ive2GQiKdrew"
      },
      "outputs": [],
      "source": [
        "# Let's train the full pipelines\n",
        "print(\"=== Training Models with Hyperparameter Tuning ===\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "best_models = {}\n",
        "grid_search_results = {}\n",
        "\n",
        "for model_name, model_info in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    \n",
        "    # Create pipeline\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model_info['model'])\n",
        "    ])\n",
        "    \n",
        "    # Perform grid search with cross-validation\n",
        "    # Improved parameter grids for better performance\n",
        "    if model_name == 'RandomForest':\n",
        "        param_grid = {\n",
        "            'classifier__n_estimators': [200, 300, 500],\n",
        "            'classifier__max_depth': [7, 10, 15, None],\n",
        "            'classifier__min_samples_split': [2, 3, 5],\n",
        "            'classifier__min_samples_leaf': [1, 2, 3],\n",
        "            'classifier__max_features': ['sqrt', 'log2'],\n",
        "            'classifier__bootstrap': [True]\n",
        "        }\n",
        "    elif model_name == 'GradientBoosting':\n",
        "        param_grid = {\n",
        "            'classifier__n_estimators': [200, 300, 400],\n",
        "            'classifier__learning_rate': [0.05, 0.1, 0.15],\n",
        "            'classifier__max_depth': [3, 4, 5, 6],\n",
        "            'classifier__min_samples_split': [2, 5, 10],\n",
        "            'classifier__min_samples_leaf': [1, 2, 4],\n",
        "            'classifier__subsample': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "    else:  # XGBoost\n",
        "        param_grid = {\n",
        "            'classifier__n_estimators': [200, 300, 400],\n",
        "            'classifier__learning_rate': [0.05, 0.1, 0.15],\n",
        "            'classifier__max_depth': [3, 4, 5, 6],\n",
        "            'classifier__min_child_weight': [1, 3, 5],\n",
        "            'classifier__subsample': [0.8, 0.9],\n",
        "            'classifier__colsample_bytree': [0.8, 0.9]\n",
        "        }\n",
        "    \n",
        "    # Grid search\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline, \n",
        "        param_grid, \n",
        "        cv=cv_strategy, \n",
        "        scoring='accuracy', \n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Fit the model\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    \n",
        "    # Store results\n",
        "    best_models[model_name] = grid_search.best_estimator_\n",
        "    grid_search_results[model_name] = {\n",
        "        'best_score': grid_search.best_score_,\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'cv_results': grid_search.cv_results_\n",
        "    }\n",
        "    \n",
        "    print(f\"Best CV Score: {grid_search.best_score_:.4f}\")\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Compare models\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "for model_name, results in grid_search_results.items():\n",
        "    print(f\"{model_name}: {results['best_score']:.4f}\")\n",
        "\n",
        "# Select the best model\n",
        "best_model_name = max(grid_search_results.keys(), key=lambda k: grid_search_results[k]['best_score'])\n",
        "best_pipeline = best_models[best_model_name]\n",
        "\n",
        "print(f\"\\nBest overall model: {best_model_name}\")\n",
        "print(f\"Best CV score: {grid_search_results[best_model_name]['best_score']:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_pipeline.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "test_roc_auc = roc_auc_score(y_test, best_pipeline.predict_proba(X_test)[:, 1])\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"ROC-AUC: {test_roc_auc:.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Rejected', 'Approved']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Rejected', 'Approved'], yticklabels=['Rejected', 'Approved'])\n",
        "plt.title(f'Confusion Matrix - {best_model_name}')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mhEqflJgEKm"
      },
      "source": [
        "# Applying the Pipelines for Prediction  \n",
        "\n",
        "With the trained **data preprocessing** and **classification pipelines**, we can now apply them to the test dataset to generate predictions.  \n",
        "\n",
        "## Steps:  \n",
        "1. **Load the test dataset**: Ensure it has the same structure as the training data.  \n",
        "2. **Apply the preprocessing pipeline**: Transform the test data using the trained preprocessing steps (e.g., encoding, scaling).  \n",
        "3. **Make predictions**: Use the trained classification pipeline to predict loan approval outcomes.  \n",
        "4. **Save or submit predictions**: Store the results for further analysis or competition submission.  \n",
        "\n",
        "This structured approach ensures consistency and avoids data leakage, making the model reliable for real-world applications.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCYgIl_qf0hY",
        "outputId": "5bab17a8-2789-4a65-e083-a69e84fcdbf1"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1MMxV5EzWu4xI8i4LoBjK-q-94RsQq-mE' -O loanpred_test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0Z4AawpVgaYF",
        "outputId": "4ad43f83-eb2d-4171-f61e-2f501b07f6bc"
      },
      "outputs": [],
      "source": [
        "#Let's load the test data\n",
        "test_df = pd.read_csv('loanpred_test.csv')\n",
        "test_df.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCLTlx4lgtwb"
      },
      "outputs": [],
      "source": [
        "# Let's make predictions\n",
        "print(\"=== Making Predictions on Test Data ===\")\n",
        "\n",
        "# Check test data structure\n",
        "print(\"Test data shape:\", test_df.shape)\n",
        "print(\"Test data columns:\", test_df.columns.tolist())\n",
        "\n",
        "# Prepare test features (remove id column - note it's 'id' not 'Loan_ID' in test data)\n",
        "X_competition_test_raw = test_df.drop(['id'], axis=1)\n",
        "\n",
        "# Apply the same feature engineering to test data\n",
        "print(\"Applying feature engineering to test data...\")\n",
        "X_competition_test = add_engineered_features(X_competition_test_raw)\n",
        "\n",
        "# Make predictions using the best pipeline\n",
        "test_predictions = best_pipeline.predict(X_competition_test)\n",
        "test_probabilities = best_pipeline.predict_proba(X_competition_test)\n",
        "\n",
        "# Convert predictions back to original labels (Y/N)\n",
        "test_predictions_labels = label_encoder.inverse_transform(test_predictions)\n",
        "\n",
        "print(f\"Predictions made for {len(test_predictions)} samples\")\n",
        "print(f\"Prediction distribution:\")\n",
        "unique, counts = np.unique(test_predictions_labels, return_counts=True)\n",
        "for label, count in zip(unique, counts):\n",
        "    print(f\"  {label}: {count} ({count/len(test_predictions_labels)*100:.1f}%)\")\n",
        "\n",
        "# Show first few predictions with probabilities\n",
        "print(\"\\nFirst 10 predictions:\")\n",
        "for i in range(min(10, len(test_predictions))):\n",
        "    loan_id = test_df.iloc[i]['id']  # Use 'id' column name\n",
        "    prediction = test_predictions_labels[i]\n",
        "    prob_approved = test_probabilities[i][1]\n",
        "    print(f\"  {loan_id}: {prediction} (Prob. Approved: {prob_approved:.3f})\")\n",
        "\n",
        "print(\"Predictions completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt9JCfN6g-Qd"
      },
      "outputs": [],
      "source": [
        "# Save the results to a CSV file following the competition template\n",
        "# Convert Y/N predictions to 1/0 format as required by competition\n",
        "test_predictions_binary = np.where(test_predictions_labels == 'Y', 1, 0)\n",
        "\n",
        "# Create submission DataFrame with correct format\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_df['id'],  # Use 'id' column name from test data\n",
        "    'pred': test_predictions_binary  # Use 'pred' column name and binary format (1/0)\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission_filename = 'loan_prediction_submission.csv'\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"=== Submission File Created ===\")\n",
        "print(f\"File saved as: {submission_filename}\")\n",
        "print(f\"Submission shape: {submission.shape}\")\n",
        "print(\"\\nFirst few rows of submission:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "print(f\"\\nSubmission summary:\")\n",
        "print(submission['pred'].value_counts())\n",
        "print(f\"Approval rate: {(submission['pred'] == 1).mean()*100:.1f}%\")\n",
        "print(f\"\\nFormat: 1 = Approved, 0 = Not Approved\")\n",
        "\n",
        "# Also save the model for future use\n",
        "model_filename = f'best_loan_prediction_model_{best_model_name.lower()}.joblib'\n",
        "joblib.dump(best_pipeline, model_filename)\n",
        "print(f\"\\nBest model saved as: {model_filename}\")\n",
        "\n",
        "print(\"Submission ready for competition!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdGQ0tQbj9QR"
      },
      "source": [
        "# Explaining the Model with SHAP  \n",
        "\n",
        "Understanding how a machine learning model makes predictions is crucial, especially in applications like loan approval, where fairness and transparency are key. **SHAP (SHapley Additive Explanations)** provides a way to interpret the contribution of each feature to a model’s predictions.  \n",
        "\n",
        "## Why is SHAP Important?  \n",
        "1. **Improves Trust and Transparency**: Helps explain why a loan was approved or rejected, making the decision process clearer.  \n",
        "2. **Identifies Key Features**: Highlights which factors influence predictions the most, allowing for better feature selection and model refinement.  \n",
        "3. **Detects Bias and Unfairness**: Reveals if certain features (e.g., gender, income) have unintended strong effects on decisions.  \n",
        "4. **Enhances Model Debugging**: Helps diagnose issues like overfitting or unexpected feature dependencies.  \n",
        "\n",
        "By using SHAP, we ensure that our model is interpretable and aligned with ethical and regulatory standards.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "potM6jtKkAz5"
      },
      "outputs": [],
      "source": [
        "# Apply shap to explain\n",
        "print(\"=== SHAP Model Explanation ===\")\n",
        "\n",
        "# Install and import SHAP\n",
        "try:\n",
        "    import shap\n",
        "    print(\"SHAP library is available\")\n",
        "except ImportError:\n",
        "    print(\"Installing SHAP library...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"shap\"])\n",
        "    import shap\n",
        "    print(\"SHAP library installed and imported\")\n",
        "\n",
        "# Initialize SHAP explainer\n",
        "print(\"Creating SHAP explainer...\")\n",
        "\n",
        "# Get the preprocessed training data for the explainer\n",
        "X_train_processed = best_pipeline.named_steps['preprocessor'].fit_transform(X_train)\n",
        "\n",
        "# Get feature names after preprocessing\n",
        "feature_names = []\n",
        "\n",
        "# Numerical features (same names)\n",
        "feature_names.extend(numerical_features)\n",
        "\n",
        "# Categorical features (one-hot encoded)\n",
        "cat_feature_names = best_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
        "feature_names.extend(cat_feature_names)\n",
        "\n",
        "print(f\"Total features after preprocessing: {len(feature_names)}\")\n",
        "\n",
        "# Create explainer for the classifier only (after preprocessing)\n",
        "classifier = best_pipeline.named_steps['classifier']\n",
        "\n",
        "# For tree-based models, use TreeExplainer\n",
        "if best_model_name == 'RandomForest':\n",
        "    explainer = shap.TreeExplainer(classifier)\n",
        "    print(\"Using TreeExplainer for Random Forest\")\n",
        "elif best_model_name == 'GradientBoosting':\n",
        "    explainer = shap.TreeExplainer(classifier)\n",
        "    print(\"Using TreeExplainer for Gradient Boosting\")\n",
        "\n",
        "# Calculate SHAP values for a sample of the test set\n",
        "sample_size = min(50, len(X_test))\n",
        "X_test_sample = X_test.iloc[:sample_size]\n",
        "X_test_processed_sample = best_pipeline.named_steps['preprocessor'].transform(X_test_sample)\n",
        "\n",
        "print(f\"Calculating SHAP values for {sample_size} test samples...\")\n",
        "shap_values = explainer.shap_values(X_test_processed_sample)\n",
        "\n",
        "# For binary classification, we want the positive class (approved loans)\n",
        "if isinstance(shap_values, list):\n",
        "    shap_values = shap_values[1]  # Positive class\n",
        "\n",
        "# Create SHAP plots\n",
        "print(\"Creating SHAP visualizations...\")\n",
        "\n",
        "# Summary plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(shap_values, X_test_processed_sample, feature_names=feature_names, show=False)\n",
        "plt.title('SHAP Summary Plot - Feature Importance for Loan Approval')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature importance plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "shap.summary_plot(shap_values, X_test_processed_sample, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
        "plt.title('SHAP Feature Importance - Mean |SHAP value|')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show SHAP values for individual predictions\n",
        "print(\"\\n=== Individual Prediction Explanations ===\")\n",
        "for i in range(min(5, sample_size)):\n",
        "    loan_id = test_df.iloc[i]['id']  # Use 'id' column name\n",
        "    prediction = test_predictions_labels[i]\n",
        "    prob_approved = test_probabilities[i][1]\n",
        "    \n",
        "    print(f\"\\nLoan {loan_id} - Predicted: {prediction} (Prob: {prob_approved:.3f})\")\n",
        "    \n",
        "    # Get top 5 most important features for this prediction\n",
        "    feature_importance = list(zip(feature_names, shap_values[i]))\n",
        "    feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
        "    \n",
        "    print(\"Top 5 contributing features:\")\n",
        "    for j, (feature, shap_val) in enumerate(feature_importance[:5]):\n",
        "        direction = \"→ APPROVE\" if shap_val > 0 else \"→ REJECT\"\n",
        "        print(f\"  {j+1}. {feature}: {shap_val:.4f} {direction}\")\n",
        "\n",
        "# Waterfall plot for first prediction\n",
        "if sample_size > 0:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # Handle base values correctly for binary classification\n",
        "    base_value = explainer.expected_value\n",
        "    if isinstance(base_value, (list, np.ndarray)):\n",
        "        base_value = base_value[1] if len(base_value) > 1 else base_value[0]\n",
        "    \n",
        "    shap.waterfall_plot(shap.Explanation(values=shap_values[0], \n",
        "                                       base_values=base_value, \n",
        "                                       data=X_test_processed_sample[0],\n",
        "                                       feature_names=feature_names), \n",
        "                       show=False)\n",
        "    plt.title(f'SHAP Waterfall Plot - Loan {test_df.iloc[0][\"id\"]}')  # Use 'id' column name\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n=== SHAP Analysis Complete ===\")\n",
        "print(\"Key insights:\")\n",
        "print(\"- SHAP values show how each feature contributes to the final prediction\")\n",
        "print(\"- Positive SHAP values push towards loan approval\")\n",
        "print(\"- Negative SHAP values push towards loan rejection\")\n",
        "print(\"- The magnitude indicates the strength of the contribution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
